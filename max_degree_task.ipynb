{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "04116cac-3e0c-46bd-a71d-01a911029f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.nn import (\n",
    "    BatchNorm1d,\n",
    "    Embedding,\n",
    "    Linear,\n",
    "    ModuleList,\n",
    "    ReLU,\n",
    "    Sequential,\n",
    ")\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx, from_networkx, to_dense_adj\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, global_add_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "da3c44a4-6ddc-4c10-946f-6a43fba0e765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_max_degree_graph(num_nodes: int, topology: str=\"complete\",\n",
    "                              random_features: str=\"gaussian\", feature_dim: int=1) -> Data:\n",
    "    assert num_nodes > 0\n",
    "    assert topology in [\"complete\", \"path\", \"cycle\", \"regular\", \"tree\"], \"Error: unknown topology\" # need to implement more\n",
    "    assert random_features in [\"gaussian\"], \"Error: unknown feature distribution\" # need to implement more\n",
    "    assert feature_dim > 0\n",
    "    \n",
    "    # create a networkx graph with the desired topology\n",
    "    if topology == \"complete\":\n",
    "        raw_graph = create_complete_graph(num_nodes)\n",
    "        \n",
    "    if topology == \"path\":\n",
    "        raw_graph = create_path_graph(num_nodes)\n",
    "        \n",
    "    if topology == \"cycle\":\n",
    "        raw_graph = create_cycle_graph(num_nodes)\n",
    "        \n",
    "    if topology == \"regular\":\n",
    "        raw_graph = create_4_regular_grid_graph(num_nodes, num_nodes)\n",
    "        \n",
    "    if topology == \"tree\":\n",
    "        raw_graph = create_binary_tree(num_nodes)\n",
    "        \n",
    "    # add random features from the desired distribution\n",
    "    if random_features == \"gaussian\":\n",
    "        attributed_graph = add_gaussian_node_features(raw_graph, feature_dim)\n",
    "    \n",
    "    # convert the networkx graph to pytorch geometric's Data format\n",
    "    pyg_graph = from_networkx(attributed_graph)\n",
    "    \n",
    "    # add the max degree as the graph label\n",
    "    pyg_graph.y = torch.tensor([max(dict(attributed_graph.degree()).values())])\n",
    "    \n",
    "    return pyg_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "29790d86-2986-4d6d-aa3d-8a0ca91e43f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max degree task on complete graphs\n",
    "\n",
    "random_integers = np.random.randint(5, 51, size=1000)\n",
    "graphs = [generate_max_degree_graph(num_nodes=nodes, topology='tree', feature_dim=10) for nodes in random_integers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "b689342c-07eb-40cc-8de1-e0d1c84d38f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7661566772460937\n"
     ]
    }
   ],
   "source": [
    "test_dataset = graphs\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "test_mae = test(test_loader, model, device, optimizer)\n",
    "print(test_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3d846fc7-c2a5-48ed-9708-445e748c3900",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"synthetic_data/shortest_path_task/tree_graphs.pkl\"\n",
    "\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(graphs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "362f2593-1bfb-4f3e-b2c8-c939ca8c9f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topologies\n",
    "\n",
    "def create_complete_graph(num_nodes: int) -> nx.graph:\n",
    "    complete_graph = nx.complete_graph(num_nodes).to_undirected()\n",
    "    return complete_graph\n",
    "\n",
    "def create_path_graph(num_nodes: int) -> nx.Graph:\n",
    "    path_graph = nx.path_graph(num_nodes)\n",
    "    return path_graph\n",
    "\n",
    "def create_cycle_graph(num_nodes: int) -> nx.Graph:\n",
    "    cycle_graph = nx.cycle_graph(num_nodes)\n",
    "    return cycle_graph\n",
    "\n",
    "def create_4_regular_grid_graph(rows: int, cols: int) -> nx.Graph:    \n",
    "    grid_graph = nx.grid_2d_graph(rows, cols, periodic=True)  # Wraps around for 4-regular structure\n",
    "    grid_graph =  nx.convert_node_labels_to_integers(grid_graph)\n",
    "    for node in grid_graph.nodes:\n",
    "        grid_graph.nodes[node].clear()\n",
    "    return grid_graph\n",
    "\n",
    "def create_binary_tree(num_nodes: int) -> nx.Graph:\n",
    "    max_depth = math.ceil(math.log2(num_nodes + 1)) - 1\n",
    "    tree = nx.balanced_tree(r=2, h=max_depth)    \n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d0a9dee9-013d-4c16-a5bf-9e0bc31fa86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# node feature distributions\n",
    "\n",
    "def add_gaussian_node_features(G: nx.graph, k: int) -> nx.graph:\n",
    "    mean = np.zeros(k)\n",
    "    cov = np.eye(k)\n",
    "\n",
    "    for node in G.nodes():\n",
    "        G.nodes[node]['x'] = np.random.multivariate_normal(mean, cov)\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5a987e-ed73-4ae1-9b85-0446f0b51215",
   "metadata": {},
   "source": [
    "## Train an example model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f5fb4b3e-b947-40fe-a8b8-0fbbe15cc7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, channels, num_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.node_emb = Linear(10, channels)\n",
    "        self.pe_norm = BatchNorm1d(20)\n",
    "        self.edge_emb = Linear(3, channels)\n",
    "        \n",
    "        self.convs = ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = GCNConv(channels, channels, normalize=True)\n",
    "            self.convs.append(conv)       \n",
    "            \n",
    "        self.mlp = Sequential(\n",
    "            Linear(channels, channels),\n",
    "            ReLU(),\n",
    "            Linear(channels, channels // 2),\n",
    "            ReLU(),\n",
    "            Linear(channels // 2, channels // 4),\n",
    "            ReLU(),\n",
    "            Linear(channels // 4, 1),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        # dropout = Dropout(0.5)\n",
    "        x = x.float()\n",
    "        x = self.node_emb(x.squeeze(-1))\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            # x = dropout(x)\n",
    "        x = global_add_pool(x, batch)\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1726a0f8-a8e5-4cbf-8d85-89ada591af21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, device, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # model.redraw_projection.redraw_projections()\n",
    "        # out = model(data.x, data.pe, data.edge_index, data.edge_attr, data.batch)\n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        loss = (out.squeeze() - data.y).abs().mean()\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader, model, device, optimizer):\n",
    "    model.eval()\n",
    "\n",
    "    total_error = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        # out = model(data.x, data.pe, data.edge_index, data.edge_attr, data.batch)\n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        total_error += (out.squeeze() - data.y).abs().sum().item()\n",
    "    return total_error / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "9aa87db2-9809-4941-adcb-547b546f48c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN(channels=64, num_layers=4).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10,\n",
    "                              min_lr=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d0a6f397-eb3a-4ccf-9d49-5efd949c6dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = graphs[:500]\n",
    "val_dataset = graphs[500:750]\n",
    "test_dataset = graphs[750:]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d796a595-ae0b-49fe-9357-e168775fb162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 2.3040, Val: 2.1178, Test: 2.0050\n",
      "Epoch: 02, Loss: 1.9573, Val: 1.9108, Test: 1.8201\n",
      "Epoch: 03, Loss: 1.8789, Val: 1.8988, Test: 1.7927\n",
      "Epoch: 04, Loss: 1.8162, Val: 1.8507, Test: 1.7591\n",
      "Epoch: 05, Loss: 1.7779, Val: 1.8430, Test: 1.7488\n",
      "Epoch: 06, Loss: 1.7705, Val: 1.8808, Test: 1.7951\n",
      "Epoch: 07, Loss: 1.7588, Val: 1.8148, Test: 1.7240\n",
      "Epoch: 08, Loss: 1.7043, Val: 1.7990, Test: 1.7241\n",
      "Epoch: 09, Loss: 1.6925, Val: 1.7957, Test: 1.7299\n",
      "Epoch: 10, Loss: 1.6690, Val: 1.7598, Test: 1.6940\n",
      "Epoch: 11, Loss: 1.6329, Val: 1.7392, Test: 1.6917\n",
      "Epoch: 12, Loss: 1.5993, Val: 1.7256, Test: 1.6795\n",
      "Epoch: 13, Loss: 1.5773, Val: 1.6919, Test: 1.6724\n",
      "Epoch: 14, Loss: 1.5468, Val: 1.6823, Test: 1.6397\n",
      "Epoch: 15, Loss: 1.5107, Val: 1.6429, Test: 1.6305\n",
      "Epoch: 16, Loss: 1.4921, Val: 1.6578, Test: 1.6276\n",
      "Epoch: 17, Loss: 1.4680, Val: 1.6206, Test: 1.5994\n",
      "Epoch: 18, Loss: 1.4220, Val: 1.6347, Test: 1.6288\n",
      "Epoch: 19, Loss: 1.4108, Val: 1.5560, Test: 1.5921\n",
      "Epoch: 20, Loss: 1.3639, Val: 1.5441, Test: 1.5677\n",
      "Epoch: 21, Loss: 1.3472, Val: 1.5292, Test: 1.5804\n",
      "Epoch: 22, Loss: 1.3281, Val: 1.4977, Test: 1.5237\n",
      "Epoch: 23, Loss: 1.2872, Val: 1.4511, Test: 1.4796\n",
      "Epoch: 24, Loss: 1.2444, Val: 1.4291, Test: 1.5051\n",
      "Epoch: 25, Loss: 1.2346, Val: 1.4275, Test: 1.4622\n",
      "Epoch: 26, Loss: 1.2193, Val: 1.3881, Test: 1.4320\n",
      "Epoch: 27, Loss: 1.2071, Val: 1.3825, Test: 1.4598\n",
      "Epoch: 28, Loss: 1.2125, Val: 1.3534, Test: 1.4265\n",
      "Epoch: 29, Loss: 1.1719, Val: 1.3396, Test: 1.3994\n",
      "Epoch: 30, Loss: 1.1415, Val: 1.3293, Test: 1.4040\n",
      "Epoch: 31, Loss: 1.1485, Val: 1.3160, Test: 1.3995\n",
      "Epoch: 32, Loss: 1.1286, Val: 1.2996, Test: 1.4188\n",
      "Epoch: 33, Loss: 1.1010, Val: 1.3013, Test: 1.4033\n",
      "Epoch: 34, Loss: 1.1192, Val: 1.2804, Test: 1.3889\n",
      "Epoch: 35, Loss: 1.0683, Val: 1.2806, Test: 1.3856\n",
      "Epoch: 36, Loss: 1.0582, Val: 1.2553, Test: 1.3949\n",
      "Epoch: 37, Loss: 1.0713, Val: 1.2516, Test: 1.3871\n",
      "Epoch: 38, Loss: 1.0375, Val: 1.2661, Test: 1.3712\n",
      "Epoch: 39, Loss: 1.0322, Val: 1.2198, Test: 1.3731\n",
      "Epoch: 40, Loss: 1.0443, Val: 1.2814, Test: 1.3458\n",
      "Epoch: 41, Loss: 1.0435, Val: 1.2475, Test: 1.3746\n",
      "Epoch: 42, Loss: 1.0256, Val: 1.2239, Test: 1.3865\n",
      "Epoch: 43, Loss: 0.9825, Val: 1.2109, Test: 1.3282\n",
      "Epoch: 44, Loss: 0.9858, Val: 1.2016, Test: 1.3041\n",
      "Epoch: 45, Loss: 0.9913, Val: 1.2107, Test: 1.3405\n",
      "Epoch: 46, Loss: 0.9647, Val: 1.1858, Test: 1.3040\n",
      "Epoch: 47, Loss: 0.9672, Val: 1.1891, Test: 1.3075\n",
      "Epoch: 48, Loss: 0.9573, Val: 1.1887, Test: 1.2753\n",
      "Epoch: 49, Loss: 0.9928, Val: 1.2148, Test: 1.2974\n",
      "Epoch: 50, Loss: 0.9564, Val: 1.1501, Test: 1.2583\n",
      "Epoch: 51, Loss: 0.9227, Val: 1.1741, Test: 1.2674\n",
      "Epoch: 52, Loss: 0.9343, Val: 1.1431, Test: 1.2451\n",
      "Epoch: 53, Loss: 0.9184, Val: 1.1420, Test: 1.2443\n",
      "Epoch: 54, Loss: 0.9327, Val: 1.1551, Test: 1.2420\n",
      "Epoch: 55, Loss: 0.9066, Val: 1.1310, Test: 1.2382\n",
      "Epoch: 56, Loss: 0.8833, Val: 1.1529, Test: 1.2348\n",
      "Epoch: 57, Loss: 0.8981, Val: 1.2019, Test: 1.2713\n",
      "Epoch: 58, Loss: 0.8983, Val: 1.1331, Test: 1.2261\n",
      "Epoch: 59, Loss: 0.8671, Val: 1.1415, Test: 1.2244\n",
      "Epoch: 60, Loss: 0.8721, Val: 1.1106, Test: 1.2161\n",
      "Epoch: 61, Loss: 0.8712, Val: 1.1441, Test: 1.2009\n",
      "Epoch: 62, Loss: 0.8624, Val: 1.1029, Test: 1.1966\n",
      "Epoch: 63, Loss: 0.8640, Val: 1.1235, Test: 1.1906\n",
      "Epoch: 64, Loss: 0.8450, Val: 1.1237, Test: 1.1892\n",
      "Epoch: 65, Loss: 0.8554, Val: 1.1571, Test: 1.2134\n",
      "Epoch: 66, Loss: 0.8479, Val: 1.1179, Test: 1.1686\n",
      "Epoch: 67, Loss: 0.8347, Val: 1.1049, Test: 1.1455\n",
      "Epoch: 68, Loss: 0.8281, Val: 1.1211, Test: 1.1583\n",
      "Epoch: 69, Loss: 0.8290, Val: 1.0963, Test: 1.1445\n",
      "Epoch: 70, Loss: 0.8348, Val: 1.1056, Test: 1.1480\n",
      "Epoch: 71, Loss: 0.8175, Val: 1.0945, Test: 1.1277\n",
      "Epoch: 72, Loss: 0.8364, Val: 1.0685, Test: 1.1235\n",
      "Epoch: 73, Loss: 0.8271, Val: 1.0881, Test: 1.1340\n",
      "Epoch: 74, Loss: 0.8068, Val: 1.0821, Test: 1.1252\n",
      "Epoch: 75, Loss: 0.7919, Val: 1.0990, Test: 1.1380\n",
      "Epoch: 76, Loss: 0.8019, Val: 1.1134, Test: 1.1575\n",
      "Epoch: 77, Loss: 0.7964, Val: 1.0716, Test: 1.1256\n",
      "Epoch: 78, Loss: 0.8116, Val: 1.0990, Test: 1.1259\n",
      "Epoch: 79, Loss: 0.8011, Val: 1.0590, Test: 1.1038\n",
      "Epoch: 80, Loss: 0.7803, Val: 1.0930, Test: 1.1173\n",
      "Epoch: 81, Loss: 0.7748, Val: 1.0637, Test: 1.0982\n",
      "Epoch: 82, Loss: 0.7656, Val: 1.0613, Test: 1.0878\n",
      "Epoch: 83, Loss: 0.7812, Val: 1.0667, Test: 1.0808\n",
      "Epoch: 84, Loss: 0.7664, Val: 1.0698, Test: 1.0945\n",
      "Epoch: 85, Loss: 0.7731, Val: 1.1093, Test: 1.1151\n",
      "Epoch: 86, Loss: 0.7717, Val: 1.0583, Test: 1.0680\n",
      "Epoch: 87, Loss: 0.7547, Val: 1.0500, Test: 1.0759\n",
      "Epoch: 88, Loss: 0.7472, Val: 1.0517, Test: 1.0693\n",
      "Epoch: 89, Loss: 0.7524, Val: 1.0248, Test: 1.0538\n",
      "Epoch: 90, Loss: 0.7365, Val: 1.0465, Test: 1.0521\n",
      "Epoch: 91, Loss: 0.7387, Val: 1.0279, Test: 1.0359\n",
      "Epoch: 92, Loss: 0.7267, Val: 1.0220, Test: 1.0418\n",
      "Epoch: 93, Loss: 0.7488, Val: 1.0381, Test: 1.0320\n",
      "Epoch: 94, Loss: 0.7202, Val: 1.0377, Test: 1.0605\n",
      "Epoch: 95, Loss: 0.7351, Val: 1.0618, Test: 1.0581\n",
      "Epoch: 96, Loss: 0.7368, Val: 1.0366, Test: 1.0400\n",
      "Epoch: 97, Loss: 0.7216, Val: 1.0274, Test: 1.0237\n",
      "Epoch: 98, Loss: 0.6994, Val: 1.0299, Test: 1.0485\n",
      "Epoch: 99, Loss: 0.7104, Val: 1.0136, Test: 1.0166\n",
      "Epoch: 100, Loss: 0.7097, Val: 1.0112, Test: 1.0189\n",
      "Epoch: 101, Loss: 0.6865, Val: 1.0111, Test: 1.0198\n",
      "Epoch: 102, Loss: 0.6906, Val: 1.0101, Test: 1.0140\n",
      "Epoch: 103, Loss: 0.6830, Val: 1.0063, Test: 1.0110\n",
      "Epoch: 104, Loss: 0.6908, Val: 1.0068, Test: 1.0163\n",
      "Epoch: 105, Loss: 0.7276, Val: 1.0025, Test: 1.0093\n",
      "Epoch: 106, Loss: 0.6933, Val: 0.9865, Test: 1.0071\n",
      "Epoch: 107, Loss: 0.6943, Val: 0.9949, Test: 1.0175\n",
      "Epoch: 108, Loss: 0.7003, Val: 1.0107, Test: 0.9994\n",
      "Epoch: 109, Loss: 0.6878, Val: 1.0312, Test: 1.0347\n",
      "Epoch: 110, Loss: 0.7074, Val: 1.0074, Test: 1.0178\n",
      "Epoch: 111, Loss: 0.6844, Val: 0.9965, Test: 1.0019\n",
      "Epoch: 112, Loss: 0.6666, Val: 1.0014, Test: 1.0147\n",
      "Epoch: 113, Loss: 0.6702, Val: 1.0060, Test: 0.9953\n",
      "Epoch: 114, Loss: 0.6780, Val: 0.9900, Test: 1.0037\n",
      "Epoch: 115, Loss: 0.6631, Val: 1.0028, Test: 1.0021\n",
      "Epoch: 116, Loss: 0.6559, Val: 1.0015, Test: 1.0187\n",
      "Epoch: 117, Loss: 0.6671, Val: 0.9898, Test: 0.9839\n",
      "Epoch: 118, Loss: 0.6327, Val: 0.9837, Test: 0.9863\n",
      "Epoch: 119, Loss: 0.6303, Val: 0.9750, Test: 0.9790\n",
      "Epoch: 120, Loss: 0.6269, Val: 0.9759, Test: 0.9830\n",
      "Epoch: 121, Loss: 0.6214, Val: 0.9744, Test: 0.9828\n",
      "Epoch: 122, Loss: 0.6221, Val: 0.9763, Test: 0.9837\n",
      "Epoch: 123, Loss: 0.6198, Val: 0.9730, Test: 0.9849\n",
      "Epoch: 124, Loss: 0.6235, Val: 0.9694, Test: 0.9768\n",
      "Epoch: 125, Loss: 0.6156, Val: 0.9688, Test: 0.9756\n",
      "Epoch: 126, Loss: 0.6126, Val: 0.9595, Test: 0.9760\n",
      "Epoch: 127, Loss: 0.6235, Val: 0.9612, Test: 0.9693\n",
      "Epoch: 128, Loss: 0.6142, Val: 0.9749, Test: 0.9806\n",
      "Epoch: 129, Loss: 0.6218, Val: 0.9626, Test: 0.9751\n",
      "Epoch: 130, Loss: 0.6151, Val: 0.9744, Test: 0.9818\n",
      "Epoch: 131, Loss: 0.6110, Val: 0.9628, Test: 0.9725\n",
      "Epoch: 132, Loss: 0.6090, Val: 0.9674, Test: 0.9724\n",
      "Epoch: 133, Loss: 0.6046, Val: 0.9580, Test: 0.9659\n",
      "Epoch: 134, Loss: 0.6083, Val: 0.9512, Test: 0.9736\n",
      "Epoch: 135, Loss: 0.6003, Val: 0.9684, Test: 0.9749\n",
      "Epoch: 136, Loss: 0.5932, Val: 0.9658, Test: 0.9802\n",
      "Epoch: 137, Loss: 0.6075, Val: 0.9644, Test: 0.9761\n",
      "Epoch: 138, Loss: 0.5982, Val: 0.9681, Test: 0.9804\n",
      "Epoch: 139, Loss: 0.6079, Val: 0.9627, Test: 0.9748\n",
      "Epoch: 140, Loss: 0.5931, Val: 0.9477, Test: 0.9682\n",
      "Epoch: 141, Loss: 0.5925, Val: 0.9505, Test: 0.9679\n",
      "Epoch: 142, Loss: 0.5950, Val: 0.9457, Test: 0.9651\n",
      "Epoch: 143, Loss: 0.5893, Val: 0.9489, Test: 0.9676\n",
      "Epoch: 144, Loss: 0.5936, Val: 0.9499, Test: 0.9638\n",
      "Epoch: 145, Loss: 0.5821, Val: 0.9459, Test: 0.9628\n",
      "Epoch: 146, Loss: 0.5908, Val: 0.9455, Test: 0.9691\n",
      "Epoch: 147, Loss: 0.5889, Val: 0.9419, Test: 0.9517\n",
      "Epoch: 148, Loss: 0.5831, Val: 0.9492, Test: 0.9674\n",
      "Epoch: 149, Loss: 0.5796, Val: 0.9460, Test: 0.9586\n",
      "Epoch: 150, Loss: 0.5778, Val: 0.9610, Test: 0.9735\n",
      "Epoch: 151, Loss: 0.5919, Val: 0.9438, Test: 0.9622\n",
      "Epoch: 152, Loss: 0.5818, Val: 0.9500, Test: 0.9653\n",
      "Epoch: 153, Loss: 0.5756, Val: 0.9346, Test: 0.9627\n",
      "Epoch: 154, Loss: 0.5754, Val: 0.9333, Test: 0.9647\n",
      "Epoch: 155, Loss: 0.5856, Val: 0.9546, Test: 0.9714\n",
      "Epoch: 156, Loss: 0.5778, Val: 0.9291, Test: 0.9596\n",
      "Epoch: 157, Loss: 0.5716, Val: 0.9347, Test: 0.9629\n",
      "Epoch: 158, Loss: 0.5651, Val: 0.9337, Test: 0.9561\n",
      "Epoch: 159, Loss: 0.5701, Val: 0.9230, Test: 0.9596\n",
      "Epoch: 160, Loss: 0.5874, Val: 0.9325, Test: 0.9561\n",
      "Epoch: 161, Loss: 0.5680, Val: 0.9289, Test: 0.9563\n",
      "Epoch: 162, Loss: 0.5822, Val: 0.9289, Test: 0.9576\n",
      "Epoch: 163, Loss: 0.5785, Val: 0.9369, Test: 0.9574\n",
      "Epoch: 164, Loss: 0.5630, Val: 0.9355, Test: 0.9563\n",
      "Epoch: 165, Loss: 0.5686, Val: 0.9398, Test: 0.9614\n",
      "Epoch: 166, Loss: 0.5572, Val: 0.9304, Test: 0.9529\n",
      "Epoch: 167, Loss: 0.5560, Val: 0.9277, Test: 0.9524\n",
      "Epoch: 168, Loss: 0.5621, Val: 0.9278, Test: 0.9596\n",
      "Epoch: 169, Loss: 0.5608, Val: 0.9302, Test: 0.9438\n",
      "Epoch: 170, Loss: 0.5644, Val: 0.9251, Test: 0.9536\n",
      "Epoch: 171, Loss: 0.5505, Val: 0.9227, Test: 0.9434\n",
      "Epoch: 172, Loss: 0.5428, Val: 0.9239, Test: 0.9466\n",
      "Epoch: 173, Loss: 0.5476, Val: 0.9169, Test: 0.9459\n",
      "Epoch: 174, Loss: 0.5471, Val: 0.9416, Test: 0.9584\n",
      "Epoch: 175, Loss: 0.5474, Val: 0.9256, Test: 0.9538\n",
      "Epoch: 176, Loss: 0.5414, Val: 0.9184, Test: 0.9502\n",
      "Epoch: 177, Loss: 0.5374, Val: 0.9256, Test: 0.9538\n",
      "Epoch: 178, Loss: 0.5393, Val: 0.9252, Test: 0.9477\n",
      "Epoch: 179, Loss: 0.5387, Val: 0.9201, Test: 0.9542\n",
      "Epoch: 180, Loss: 0.5403, Val: 0.9177, Test: 0.9474\n",
      "Epoch: 181, Loss: 0.5385, Val: 0.9211, Test: 0.9488\n",
      "Epoch: 182, Loss: 0.5423, Val: 0.9239, Test: 0.9515\n",
      "Epoch: 183, Loss: 0.5333, Val: 0.9185, Test: 0.9483\n",
      "Epoch: 184, Loss: 0.5403, Val: 0.9213, Test: 0.9518\n",
      "Epoch: 185, Loss: 0.5330, Val: 0.9122, Test: 0.9458\n",
      "Epoch: 186, Loss: 0.5344, Val: 0.9230, Test: 0.9492\n",
      "Epoch: 187, Loss: 0.5324, Val: 0.9180, Test: 0.9485\n",
      "Epoch: 188, Loss: 0.5317, Val: 0.9148, Test: 0.9450\n",
      "Epoch: 189, Loss: 0.5266, Val: 0.9228, Test: 0.9478\n",
      "Epoch: 190, Loss: 0.5285, Val: 0.9191, Test: 0.9475\n",
      "Epoch: 191, Loss: 0.5260, Val: 0.9169, Test: 0.9453\n",
      "Epoch: 192, Loss: 0.5249, Val: 0.9210, Test: 0.9479\n",
      "Epoch: 193, Loss: 0.5273, Val: 0.9167, Test: 0.9469\n",
      "Epoch: 194, Loss: 0.5253, Val: 0.9184, Test: 0.9457\n",
      "Epoch: 195, Loss: 0.5266, Val: 0.9140, Test: 0.9449\n",
      "Epoch: 196, Loss: 0.5299, Val: 0.9218, Test: 0.9485\n",
      "Epoch: 197, Loss: 0.5251, Val: 0.9167, Test: 0.9448\n",
      "Epoch: 198, Loss: 0.5225, Val: 0.9174, Test: 0.9459\n",
      "Epoch: 199, Loss: 0.5216, Val: 0.9154, Test: 0.9446\n",
      "Epoch: 200, Loss: 0.5210, Val: 0.9175, Test: 0.9452\n",
      "Epoch: 201, Loss: 0.5218, Val: 0.9161, Test: 0.9448\n",
      "Epoch: 202, Loss: 0.5223, Val: 0.9175, Test: 0.9451\n",
      "Epoch: 203, Loss: 0.5212, Val: 0.9169, Test: 0.9464\n",
      "Epoch: 204, Loss: 0.5206, Val: 0.9169, Test: 0.9450\n",
      "Epoch: 205, Loss: 0.5199, Val: 0.9168, Test: 0.9454\n",
      "Epoch: 206, Loss: 0.5198, Val: 0.9176, Test: 0.9455\n",
      "Epoch: 207, Loss: 0.5205, Val: 0.9184, Test: 0.9457\n",
      "Epoch: 208, Loss: 0.5194, Val: 0.9150, Test: 0.9444\n",
      "Epoch: 209, Loss: 0.5181, Val: 0.9170, Test: 0.9450\n",
      "Epoch: 210, Loss: 0.5184, Val: 0.9160, Test: 0.9452\n",
      "Epoch: 211, Loss: 0.5179, Val: 0.9165, Test: 0.9450\n",
      "Epoch: 212, Loss: 0.5177, Val: 0.9156, Test: 0.9450\n",
      "Epoch: 213, Loss: 0.5176, Val: 0.9161, Test: 0.9450\n",
      "Epoch: 214, Loss: 0.5176, Val: 0.9153, Test: 0.9447\n",
      "Epoch: 215, Loss: 0.5181, Val: 0.9160, Test: 0.9449\n",
      "Epoch: 216, Loss: 0.5180, Val: 0.9163, Test: 0.9445\n",
      "Epoch: 217, Loss: 0.5177, Val: 0.9160, Test: 0.9448\n",
      "Epoch: 218, Loss: 0.5177, Val: 0.9149, Test: 0.9442\n",
      "Epoch: 219, Loss: 0.5170, Val: 0.9157, Test: 0.9447\n",
      "Epoch: 220, Loss: 0.5167, Val: 0.9159, Test: 0.9444\n",
      "Epoch: 221, Loss: 0.5167, Val: 0.9158, Test: 0.9446\n",
      "Epoch: 222, Loss: 0.5165, Val: 0.9159, Test: 0.9447\n",
      "Epoch: 223, Loss: 0.5166, Val: 0.9154, Test: 0.9446\n",
      "Epoch: 224, Loss: 0.5164, Val: 0.9160, Test: 0.9447\n",
      "Epoch: 225, Loss: 0.5161, Val: 0.9153, Test: 0.9443\n",
      "Epoch: 226, Loss: 0.5164, Val: 0.9153, Test: 0.9444\n",
      "Epoch: 227, Loss: 0.5162, Val: 0.9156, Test: 0.9446\n",
      "Epoch: 228, Loss: 0.5161, Val: 0.9156, Test: 0.9445\n",
      "Epoch: 229, Loss: 0.5160, Val: 0.9154, Test: 0.9445\n",
      "Epoch: 230, Loss: 0.5157, Val: 0.9153, Test: 0.9444\n",
      "Epoch: 231, Loss: 0.5158, Val: 0.9152, Test: 0.9445\n",
      "Epoch: 232, Loss: 0.5157, Val: 0.9154, Test: 0.9444\n",
      "Epoch: 233, Loss: 0.5156, Val: 0.9154, Test: 0.9444\n",
      "Epoch: 234, Loss: 0.5155, Val: 0.9153, Test: 0.9444\n",
      "Epoch: 235, Loss: 0.5156, Val: 0.9154, Test: 0.9446\n",
      "Epoch: 236, Loss: 0.5156, Val: 0.9152, Test: 0.9440\n",
      "Epoch: 237, Loss: 0.5154, Val: 0.9152, Test: 0.9444\n",
      "Epoch: 238, Loss: 0.5156, Val: 0.9153, Test: 0.9443\n",
      "Epoch: 239, Loss: 0.5157, Val: 0.9152, Test: 0.9442\n",
      "Epoch: 240, Loss: 0.5155, Val: 0.9152, Test: 0.9444\n",
      "Epoch: 241, Loss: 0.5153, Val: 0.9152, Test: 0.9444\n",
      "Epoch: 242, Loss: 0.5153, Val: 0.9153, Test: 0.9443\n",
      "Epoch: 243, Loss: 0.5156, Val: 0.9148, Test: 0.9440\n",
      "Epoch: 244, Loss: 0.5151, Val: 0.9154, Test: 0.9445\n",
      "Epoch: 245, Loss: 0.5152, Val: 0.9153, Test: 0.9444\n",
      "Epoch: 246, Loss: 0.5152, Val: 0.9153, Test: 0.9444\n",
      "Epoch: 247, Loss: 0.5152, Val: 0.9150, Test: 0.9441\n",
      "Epoch: 248, Loss: 0.5152, Val: 0.9154, Test: 0.9444\n",
      "Epoch: 249, Loss: 0.5154, Val: 0.9151, Test: 0.9443\n",
      "Epoch: 250, Loss: 0.5152, Val: 0.9149, Test: 0.9444\n",
      "Epoch: 251, Loss: 0.5150, Val: 0.9151, Test: 0.9442\n",
      "Epoch: 252, Loss: 0.5150, Val: 0.9148, Test: 0.9440\n",
      "Epoch: 253, Loss: 0.5150, Val: 0.9150, Test: 0.9442\n",
      "Epoch: 254, Loss: 0.5149, Val: 0.9149, Test: 0.9442\n",
      "Epoch: 255, Loss: 0.5149, Val: 0.9152, Test: 0.9442\n",
      "Epoch: 256, Loss: 0.5148, Val: 0.9149, Test: 0.9442\n",
      "Epoch: 257, Loss: 0.5147, Val: 0.9150, Test: 0.9442\n",
      "Epoch: 258, Loss: 0.5149, Val: 0.9149, Test: 0.9441\n",
      "Epoch: 259, Loss: 0.5148, Val: 0.9151, Test: 0.9441\n",
      "Epoch: 260, Loss: 0.5148, Val: 0.9148, Test: 0.9441\n",
      "Epoch: 261, Loss: 0.5147, Val: 0.9147, Test: 0.9440\n",
      "Epoch: 262, Loss: 0.5147, Val: 0.9150, Test: 0.9442\n",
      "Epoch: 263, Loss: 0.5148, Val: 0.9149, Test: 0.9442\n",
      "Epoch: 264, Loss: 0.5145, Val: 0.9146, Test: 0.9440\n",
      "Epoch: 265, Loss: 0.5147, Val: 0.9149, Test: 0.9440\n",
      "Epoch: 266, Loss: 0.5145, Val: 0.9149, Test: 0.9441\n",
      "Epoch: 267, Loss: 0.5146, Val: 0.9145, Test: 0.9436\n",
      "Epoch: 268, Loss: 0.5145, Val: 0.9149, Test: 0.9440\n",
      "Epoch: 269, Loss: 0.5144, Val: 0.9147, Test: 0.9440\n",
      "Epoch: 270, Loss: 0.5145, Val: 0.9145, Test: 0.9441\n",
      "Epoch: 271, Loss: 0.5144, Val: 0.9147, Test: 0.9441\n",
      "Epoch: 272, Loss: 0.5143, Val: 0.9146, Test: 0.9439\n",
      "Epoch: 273, Loss: 0.5144, Val: 0.9145, Test: 0.9438\n",
      "Epoch: 274, Loss: 0.5144, Val: 0.9145, Test: 0.9439\n",
      "Epoch: 275, Loss: 0.5143, Val: 0.9148, Test: 0.9439\n",
      "Epoch: 276, Loss: 0.5144, Val: 0.9147, Test: 0.9439\n",
      "Epoch: 277, Loss: 0.5144, Val: 0.9147, Test: 0.9440\n",
      "Epoch: 278, Loss: 0.5142, Val: 0.9144, Test: 0.9438\n",
      "Epoch: 279, Loss: 0.5142, Val: 0.9149, Test: 0.9440\n",
      "Epoch: 280, Loss: 0.5143, Val: 0.9143, Test: 0.9440\n",
      "Epoch: 281, Loss: 0.5140, Val: 0.9145, Test: 0.9438\n",
      "Epoch: 282, Loss: 0.5140, Val: 0.9142, Test: 0.9437\n",
      "Epoch: 283, Loss: 0.5140, Val: 0.9145, Test: 0.9437\n",
      "Epoch: 284, Loss: 0.5139, Val: 0.9143, Test: 0.9437\n",
      "Epoch: 285, Loss: 0.5140, Val: 0.9143, Test: 0.9438\n",
      "Epoch: 286, Loss: 0.5138, Val: 0.9142, Test: 0.9439\n",
      "Epoch: 287, Loss: 0.5138, Val: 0.9144, Test: 0.9436\n",
      "Epoch: 288, Loss: 0.5139, Val: 0.9141, Test: 0.9438\n",
      "Epoch: 289, Loss: 0.5138, Val: 0.9142, Test: 0.9438\n",
      "Epoch: 290, Loss: 0.5138, Val: 0.9142, Test: 0.9439\n",
      "Epoch: 291, Loss: 0.5138, Val: 0.9142, Test: 0.9437\n",
      "Epoch: 292, Loss: 0.5137, Val: 0.9144, Test: 0.9439\n",
      "Epoch: 293, Loss: 0.5137, Val: 0.9139, Test: 0.9436\n",
      "Epoch: 294, Loss: 0.5138, Val: 0.9142, Test: 0.9435\n",
      "Epoch: 295, Loss: 0.5139, Val: 0.9138, Test: 0.9438\n",
      "Epoch: 296, Loss: 0.5136, Val: 0.9144, Test: 0.9434\n",
      "Epoch: 297, Loss: 0.5140, Val: 0.9147, Test: 0.9434\n",
      "Epoch: 298, Loss: 0.5138, Val: 0.9140, Test: 0.9436\n",
      "Epoch: 299, Loss: 0.5138, Val: 0.9138, Test: 0.9437\n",
      "Epoch: 300, Loss: 0.5135, Val: 0.9140, Test: 0.9435\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 301):\n",
    "    loss = train(train_loader, model, device, optimizer)\n",
    "    val_mae = test(val_loader, model, device, optimizer)\n",
    "    test_mae = test(test_loader, model, device, optimizer)\n",
    "    scheduler.step(val_mae)\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Val: {val_mae:.4f}, '\n",
    "          f'Test: {test_mae:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "borf_2",
   "language": "python",
   "name": "borf_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
